{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5.1 加减乘除 乘方开方 次方 log "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----  a+b: -----\n",
      "tensor([[ 6.,  8.],\n",
      "        [10., 12.]])\n",
      "tensor([[ 6.,  8.],\n",
      "        [10., 12.]])\n",
      "-----  a-b: -----\n",
      "tensor([[-4., -4.],\n",
      "        [-4., -4.]])\n",
      "tensor([[-4., -4.],\n",
      "        [-4., -4.]])\n",
      "-----  a*b: -----\n",
      "tensor([[ 5., 12.],\n",
      "        [21., 32.]])\n",
      "tensor([[ 5., 12.],\n",
      "        [21., 32.]])\n",
      "-----  a/b: -----\n",
      "tensor([[0.2000, 0.3333],\n",
      "        [0.4286, 0.5000]])\n",
      "tensor([[0.2000, 0.3333],\n",
      "        [0.4286, 0.5000]])\n",
      "-----  a^b: -----\n",
      "tensor([[1.0000e+00, 6.4000e+01],\n",
      "        [2.1870e+03, 6.5536e+04]])\n",
      "tensor([[1.0000e+00, 6.4000e+01],\n",
      "        [2.1870e+03, 6.5536e+04]])\n",
      "-----  long(a): -----\n",
      "tensor([[0.0000, 0.6931],\n",
      "        [1.0986, 1.3863]])\n",
      "-----  rsquare(a): -----\n",
      "tensor([[1.0000, 1.4142],\n",
      "        [1.7321, 2.0000]])\n"
     ]
    }
   ],
   "source": [
    "a = torch.FloatTensor([[1,2],[3,4]])\n",
    "b = torch.FloatTensor([[5,6],[7,8]])\n",
    "\n",
    "print(\"-----  a+b: -----\")\n",
    "print(a+b)\n",
    "print(torch.add(a,b))\n",
    "print(\"-----  a-b: -----\")\n",
    "print(a-b)\n",
    "print(torch.sub(a,b))\n",
    "print(\"-----  a*b: -----\")\n",
    "print(a*b)\n",
    "print(torch.mul(a,b))\n",
    "print(\"-----  a/b: -----\")\n",
    "print(a/b)\n",
    "print(torch.div(a,b))\n",
    "print(\"-----  a^b: -----\")\n",
    "print(a**b)\n",
    "print(torch.pow(a,b))\n",
    "print(\"-----  long(a): -----\")\n",
    "print(torch.log(a))\n",
    "print(\"-----  sqrt(a): -----\")\n",
    "print(torch.sqrt(a))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5.2 矩阵相乘"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------A dot B------\n",
      "tensor([[[19., 22.],\n",
      "         [43., 50.]]])\n",
      "tensor([[[19., 22.],\n",
      "         [43., 50.]]])\n"
     ]
    }
   ],
   "source": [
    "A = torch.FloatTensor([[[1,2],[3,4]]])\n",
    "B = torch.FloatTensor([[[5,6],[7,8]]])\n",
    "print(\"-------A dot B------\")\n",
    "print(torch.matmul(A,B))\n",
    "print(A@B)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5.3 取整，约等于，上下限制"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a:\n",
      "tensor([[0.3000, 0.4000, 0.5000, 0.6000, 1.0000, 1.4000, 1.5000, 1.7000, 2.0000,\n",
      "         2.1000]])\n",
      "------  torch.ceil(a)  -------\n",
      "tensor([[1., 1., 1., 1., 1., 2., 2., 2., 2., 3.]])\n",
      "------  torch.floor(a)  -------\n",
      "tensor([[1., 1., 1., 1., 1., 2., 2., 2., 2., 3.]])\n",
      "------  torch.round(a)  -------\n",
      "tensor([[0., 0., 0., 1., 1., 1., 2., 2., 2., 2.]])\n",
      "------  torch.trunc(a)  -------\n",
      "tensor([[0., 0., 0., 0., 1., 1., 1., 1., 2., 2.]])\n",
      "------  torch.frac(a)  -------\n",
      "tensor([[0.3000, 0.4000, 0.5000, 0.6000, 0.0000, 0.4000, 0.5000, 0.7000, 0.0000,\n",
      "         0.1000]])\n",
      "------  torch.clamp(a,1,2)  -------\n",
      "tensor([[1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.4000, 1.5000, 1.7000, 2.0000,\n",
      "         2.0000]])\n"
     ]
    }
   ],
   "source": [
    "a = torch.FloatTensor([[0.3, 0.4, 0.5, 0.6, 1, 1.4, 1.5, 1.7,2,2.1]])\n",
    "print(\"a:\")\n",
    "print(a)\n",
    "print(\"------  torch.ceil(a)  -------\")\n",
    "print(torch.ceil(a))\n",
    "print(\"------  torch.floor(a)  -------\")\n",
    "print(torch.ceil(a))\n",
    "\n",
    "print(\"------  torch.round(a)  -------\")\n",
    "print(torch.round(a))\n",
    "print(\"------  torch.trunc(a)  -------\")\n",
    "print(torch.trunc(a))\n",
    "print(\"------  torch.frac(a)  -------\")\n",
    "print(torch.frac(a))\n",
    "print(\"------  torch.clamp(a,1,2)  -------\")\n",
    "print(torch.clamp(a,1,2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5.4 统计函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------- tensor范数 torch.norm(a,2) ------\n",
      "tensor(9.5394)\n",
      "------- tensor最大值 ,最小值， ------\n",
      "tensor(6.)\n",
      "tensor(1.)\n",
      "------- tensor 均值,求和 ------\n",
      "tensor(3.5000)\n",
      "tensor([21.])\n",
      "------- tensor 最大值索引， 最小值索引 ------\n",
      "tensor(5)\n",
      "tensor(0)\n",
      "------- tensor 最大K个值 ------\n",
      "(tensor([[6., 5., 4.]]), tensor([[5, 4, 3]]))\n"
     ]
    }
   ],
   "source": [
    "a = torch.FloatTensor([[1,2,3,4,5,6]])\n",
    "print(\"------- tensor范数 torch.norm(a,2) ------\")\n",
    "print(torch.norm(a,2))\n",
    "print(\"------- tensor最大值 ,最小值， ------\")\n",
    "print(torch.max(a))\n",
    "print(torch.min(a))\n",
    "print(\"------- tensor 均值,求和 ------\")\n",
    "print(a.mean())\n",
    "print(torch.sum(a, dim=1))\n",
    "print(\"------- tensor 最大值索引， 最小值索引 ------\")\n",
    "print(a.argmax())\n",
    "print(torch.argmin(a))\n",
    "print(\"------- tensor 最大K个值 ------\")\n",
    "print(a.topk(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## torch.where(condidtion, true_state, false_state)\n",
    "## condidtion, true_state, false_state shape is the same shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cond =  tensor([[1.5433, 0.5111],\n",
      "        [0.8630, 0.6903]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[8., 8.],\n",
       "        [8., 8.]])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cond = torch.randn(2,2)\n",
    "a = torch.ones(cond.shape) * 8\n",
    "b = torch.ones(cond.shape) * 5\n",
    "print(\"cond = \", cond)\n",
    "torch.where(cond > 0,a,b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 收集器。 按index tensor对 通shape input tensor 收集样本"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[3., 2.],\n",
       "        [1., 4.]])"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.Tensor([[1,2],[3,4]]) \n",
    "i = torch.tensor([[1,0],[0,1]])\n",
    "torch.gather(a,dim=0,index = i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.5250, 0.1192, 0.8808])"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.sigmoid(torch.Tensor([0.1,-2,2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ----------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([-0.1200,  0.2373, -0.1173]),)"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.rand(3, requires_grad=True)\n",
    "p = torch.softmax(a,dim=0)\n",
    "torch.autograd.grad(p[1],[a], retain_graph=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([-0.1200,  0.2373, -0.1173]),)"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.autograd.grad(p[1],[a], retain_graph=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
